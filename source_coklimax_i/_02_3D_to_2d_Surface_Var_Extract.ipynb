{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0cfc546",
   "metadata": {},
   "source": [
    "## 3D Variable Surface Extraction\n",
    "This notebook provides a specialized tool for extracting 2D \"surface\" data from 3D PALM (Potsdam Atmospheric Large-Eddy Simulation Model) NetCDF output files. Specifically, it extracts data from the lowest non-zero atmospheric layer and the layer immediately above it for predefined groups of 3D variables (e.g., wind components). The extracted 2D data is then saved into new, smaller NetCDF files,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efb5851",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies\n",
    "This section imports all necessary Python libraries for numerical operations, NetCDF file handling, plotting (though not directly used for output here, good practice), and basic operating system interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4850b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bffd4ed",
   "metadata": {},
   "source": [
    "## 2. Load 3D Simulation Data\n",
    "This section defines the file paths for the 3D simulation output NetCDF files (for a baseline and a scenario run) and the static driver file. These files are then loaded into netCDF4 Dataset objects, providing access to the raw simulation data. The buildings_2d data from the static driver is also extracted, which might be useful for masking or contextualizing the extracted surface data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "436fca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute URLs (paths) of 3D simulation output files\n",
    "file_3d_1 = r\"./Data/_simulation_outputs_3/konstanz_4096x4096_v9_Baseline-48hr/OUTPUT/konstanz_4096x4096_v9_Baseline_av_3d_N03.000.nc\"\n",
    "file_3d_2 = r\"./Data/_simulation_outputs_3/konstanz_4096x4096_v9_Scenario_1-48hr/OUTPUT/konstanz_4096x4096_v9_Scenario_1_av_3d_N03.000.nc\"\n",
    "file_static = r\"./Data/_simulation_outputs_3/konstanz_4096x4096_v9_Scenario_1-48hr/INPUT/konstanz_4096x4096_v9_Scenario_1_static_N03\"\n",
    "\n",
    "# Read NetCDF files into Dataset objects in read mode ('r')\n",
    "dataset_1 = nc.Dataset(file_3d_1, mode='r')\n",
    "dataset_2 = nc.Dataset(file_3d_2, mode='r')\n",
    "dataset_3 = nc.Dataset(file_static, mode='r')\n",
    "\n",
    "# Store the 3D dataset objects in a list for easy iteration during processing.\n",
    "file_xy_list = [file_3d_1, file_3d_2]\n",
    "dataset_list = [dataset_1, dataset_2]\n",
    "\n",
    "# Extract 2D building data from the static driver.\n",
    "# This data might be used for masking non-atmospheric grid cells in the extracted surface data.\n",
    "buildings_2d = dataset_3['buildings_2d']\n",
    "buildings_2d_data = buildings_2d[:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aacd6dc",
   "metadata": {},
   "source": [
    "## 3. Define Time Sequences\n",
    "This section dynamically extracts the total number of time steps from the loaded dataset. It then defines parameters for time step intervals and calculates time_sequence and time_sequence_all, which are used to control which time steps are processed and included in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e523a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically get the total number of time steps from the 'time' dimension of the first dataset.\n",
    "total_time_steps = len(dataset_1.dimensions['time'])\n",
    "\n",
    "# Define `time_step_interval`: This controls how frequently data points are sampled from the full time series.\n",
    "# For example, if original data is every 10 min:\n",
    "#   - `time_step_interval = 1`: Exports every 10 min data point.\n",
    "#   - `time_step_interval = 6`: Exports every 6th data point, representing hourly data.\n",
    "time_step_interval = 6\n",
    "\n",
    "# Define `second_step`: This offset ensures that `time_sequence` starts at a meaningful interval if `time_step_interval` > 1.\n",
    "# - For `time_step_interval = 6` (hourly data), `second_step = 5` means it selects time steps corresponding to 10, 20, ..., 50, 60 minutes.\n",
    "#   Since time steps are 0-indexed and represent 10-minute intervals, step 5 corresponds to 0:50, step 11 to 1:50, etc.\n",
    "#   If the intention is to grab the hour mark (e.g., 0:00, 1:00, 2:00), adjust `second_step` based on how `time_sequence` is generated.\n",
    "if time_step_interval == 3:\n",
    "    second_step = 2\n",
    "elif time_step_interval == 6:\n",
    "    second_step = 5\n",
    "elif time_step_interval == 1:\n",
    "    second_step = 0 # Start from the first time step (index 0)\n",
    "\n",
    "# `time_sequence`: A list of selected time step indices, starting at `0` and extending by `time_step_interval`.\n",
    "# This is typically used to extract a subset of the full time series.\n",
    "time_sequence = [0]\n",
    "time_sequence.extend(np.arange(second_step, total_time_steps, time_step_interval))\n",
    "\n",
    "# `time_sequence_all`: A list containing all available time step indices (from 0 to `total_time_steps - 1`).\n",
    "# This can be used if all time steps need to be processed/exported.\n",
    "time_sequence_all = range(total_time_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19641950",
   "metadata": {},
   "source": [
    "## 4. Define Variable Groups and Helper Functions\n",
    "This section identifies all 3D variables in the dataset and categorizes them into predefined groups. It also includes helper functions to filter these groups for available variables and to extract unique dimensions for each group, which is crucial for structuring the output NetCDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1186203c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available 3D variables in dataset: ['ta', 'u', 'v', 'w', 'wspeed', 'wdir']\n",
      "\n",
      "1. ['wdir', 'wspeed']\n",
      "   Unique dimensions: ['y', 'x', 'zu_3d', 'time']\n",
      "\n",
      "2. ['u', 'v', 'w']\n",
      "   Unique dimensions: ['x', 'xu', 'zu_3d', 'time', 'zw_3d', 'yv', 'y']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all variable names available in the first dataset.\n",
    "variable_names = dataset_1.variables.keys()\n",
    "# Filter `variable_names` to include only those with more than 2 dimensions (typically 3D or 4D variables in PALM).\n",
    "variable_names_palm = [var for var in variable_names if dataset_1.variables[var].ndim > 2]\n",
    "\n",
    "print(f\"Available 3D variables in dataset: {variable_names_palm}\\n\")\n",
    "\n",
    "# Define predefined groups of variables. This allows for exporting related variables together.\n",
    "# `variable_group_1`: Typically contains wind speed and direction.\n",
    "variable_group_1 = ['wdir', 'wspeed']\n",
    "# `variable_group_2`: Typically contains Cartesian wind components.\n",
    "variable_group_2 = ['u', 'v', 'w']\n",
    "\n",
    "# Filter each predefined variable group to ensure only variables actually present in the dataset are included.\n",
    "# This prevents errors if a variable in the group is not in the loaded NetCDF file.\n",
    "for variable_group in (variable_group_1, variable_group_2):\n",
    "    updated_variable_group = []\n",
    "    for variable in variable_group:\n",
    "        if variable in variable_names_palm: # Check if the variable exists in the dataset's 3D variables.\n",
    "            updated_variable_group.append(variable)\n",
    "    variable_group.clear()  # Clear the original list.\n",
    "    variable_group.extend(updated_variable_group)  # Update the list with the filtered variables.\n",
    "\n",
    "def get_unique_dimensions(variable_group):\n",
    "    \"\"\"\n",
    "    Retrieves the unique dimensions (e.g., 'time', 'z', 'y', 'x') present across all\n",
    "    variables within a specified group. This helps in defining the structure of\n",
    "    the output NetCDF file for that group.\n",
    "\n",
    "    Args:\n",
    "        variable_group (list): A list of variable names (strings).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of unique dimension names (strings) for the variables in the group.\n",
    "    \"\"\"\n",
    "    unique_dimensions = set() # Use a set to automatically handle unique dimensions.\n",
    "    for variable_name_export in variable_group:\n",
    "        try:\n",
    "            # Get the dimensions tuple for the current variable.\n",
    "            variable_dimensions = dataset_1.variables[variable_name_export].dimensions\n",
    "            unique_dimensions.update(variable_dimensions) # Add dimensions to the set.\n",
    "        except KeyError:\n",
    "            print(f\"Warning: Variable '{variable_name_export}' not found in dataset. Skipping.\")\n",
    "    return list(unique_dimensions) # Convert the set back to a list.\n",
    "\n",
    "# Print the filtered variable groups and their unique dimensions for confirmation.\n",
    "i = 0\n",
    "for variable_group in (variable_group_1, variable_group_2):\n",
    "    i += 1 \n",
    "    unique_dimensions_group = get_unique_dimensions(variable_group)\n",
    "    print(f\"{i}. {variable_group}\")\n",
    "    print(f\"   Unique dimensions: {unique_dimensions_group}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b199c8a",
   "metadata": {},
   "source": [
    "## 5. Data Extraction Logic (2D Surface Slices)\n",
    "The subset_data function is the core of this notebook's extraction capability. It takes a 3D variable's data for a specific time step and extracts two 2D \"surface\" slices:\n",
    "1. The value at the lowest non-zero atmospheric level (often representing ground-adjacent values).\n",
    "2. The value at the next atmospheric level above the lowest non-zero level.\n",
    "\n",
    "This is particularly useful for analyzing conditions just above the ground and at a slightly higher elevation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78f6001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_data(dataset, variable_name, time_index):\n",
    "    \"\"\"\n",
    "    Extracts 2D \"surface\" data from a 3D variable for a given time step.\n",
    "    It returns two subsets:\n",
    "    - The value at the lowest non-zero atmospheric level.\n",
    "    - The value at the next atmospheric level above the lowest non-zero level.\n",
    "\n",
    "    Args:\n",
    "        dataset (netCDF4.Dataset): The NetCDF dataset containing the variable.\n",
    "        variable_name (str): The name of the 3D variable to subset (e.g., 'wspeed').\n",
    "        time_index (int): The specific time step index to extract data for.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two NumPy arrays, each of shape (1, 1, y_range, x_range),\n",
    "               representing the extracted 2D surface slices.\n",
    "    \"\"\"\n",
    "    # Get the variable data object from the dataset.\n",
    "    var_data = dataset[variable_name]\n",
    "    \n",
    "    # Determine the dimensions (levels, y_range, x_range) from the variable's shape,\n",
    "    # skipping the time dimension (index 0).\n",
    "    levels, y_range, x_range = var_data.shape[1], var_data.shape[2], var_data.shape[3]\n",
    "\n",
    "    # Initialize NumPy arrays to store the two 2D subsets.\n",
    "    # Shape (1, 1, y_range, x_range) maintains compatibility with NetCDF variable dimensions.\n",
    "    subset_0 = np.zeros((1, 1, y_range, x_range), dtype=np.float32) # For the lowest non-zero level\n",
    "    subset_1 = np.zeros((1, 1, y_range, x_range), dtype=np.float32) # For the next level\n",
    "\n",
    "    # Reshape the 3D data for the current time step into a 2D array (levels x total_spatial_points).\n",
    "    # This allows for efficient vectorized operations along the level axis.\n",
    "    flat_data = var_data[time_index, :, :, :].reshape(levels, -1)\n",
    "\n",
    "    # Find the index of the first non-zero level along the vertical (level) axis for each spatial point.\n",
    "    # `np.argmax(flat_data != 0, axis=0)` returns the first index where the condition is true.\n",
    "    # If a column is all zeros, argmax returns 0, so additional handling is needed.\n",
    "    non_zero_indices = np.argmax(flat_data != 0, axis=0)\n",
    "    \n",
    "    # Extract values at the `non_zero_indices`.\n",
    "    non_zero_values = flat_data[non_zero_indices, np.arange(flat_data.shape[1])]\n",
    "\n",
    "    # Important correction: If a spatial column (all levels at a specific y,x) is entirely zero (e.g., outside domain),\n",
    "    # `np.argmax` might still return 0, leading to incorrect non-zero values.\n",
    "    # We set these values to zero where the very first level of data is also zero.\n",
    "    # This ensures that points with no atmospheric data remain zero.\n",
    "    non_zero_values[flat_data[0] == 0] = 0\n",
    "\n",
    "    # Reshape `non_zero_values` back to the original 2D spatial dimensions (y_range, x_range)\n",
    "    # and assign it to `subset_0`.\n",
    "    subset_0[0, 0] = non_zero_values.reshape(y_range, x_range)\n",
    "\n",
    "    # Calculate the indices for the 'next level' (one level above `non_zero_indices`).\n",
    "    # `np.clip` ensures these indices do not exceed the `levels` boundary.\n",
    "    next_level_indices = np.clip(non_zero_indices + 1, 0, levels - 1)\n",
    "    \n",
    "    # Extract values at `next_level_indices`.\n",
    "    next_level_values = flat_data[next_level_indices, np.arange(flat_data.shape[1])]\n",
    "    \n",
    "    # Apply the same zero-masking for `next_level_values` to handle all-zero columns correctly.\n",
    "    next_level_values[flat_data[0] == 0] = 0\n",
    "\n",
    "    # Reshape `next_level_values` and assign it to `subset_1`.\n",
    "    subset_1[0, 0] = next_level_values.reshape(y_range, x_range)\n",
    "\n",
    "    return subset_0, subset_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d009f7",
   "metadata": {},
   "source": [
    "## 6. Select Dataset and Variable Group for Export\n",
    "This section defines which simulation dataset (dataset_1 or dataset_2) and which predefined variable group (variable_group_1 or variable_group_2) will be processed for surface data extraction. It also prepares the output filename prefix and suffix based on these selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b1b3e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `ds_index`: Selects which dataset to process.\n",
    "#   - `1` corresponds to `dataset_1` (Baseline).\n",
    "#   - `2` corresponds to `dataset_2` (Scenario 1).\n",
    "ds_index = 1\n",
    "\n",
    "# `var_group_index`: Selects which variable group to extract.\n",
    "#   - `1` corresponds to `variable_group_1` (e.g., 'wdir', 'wspeed').\n",
    "#   - `2` corresponds to `variable_group_2` (e.g., 'u', 'v', 'w').\n",
    "var_group_index = 1\n",
    "\n",
    "def get_dataset_and_filename_prefix(dataset_index):\n",
    "    \"\"\"\n",
    "    Retrieves the selected dataset object and constructs a filename prefix\n",
    "    based on the original NetCDF file's name.\n",
    "\n",
    "    Args:\n",
    "        dataset_index (int): Index indicating which dataset to select (1 for dataset_1, 2 for dataset_2).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the selected netCDF4 Dataset object and its filename prefix.\n",
    "    \"\"\"\n",
    "    if dataset_index == 1:\n",
    "        selected_dataset = dataset_1\n",
    "        # Extract filename (e.g., 'konstanz_4096x4096_v9_Baseline_av_3d_N03.000')\n",
    "        filename = os.path.basename(file_3d_1)\n",
    "        # Extract filename prefix (e.g., 'konstanz_4096x4096_v9_Baseline_av_3d_N03.000')\n",
    "        filename_prefix = filename.split(\".nc\")[0]\n",
    "    elif dataset_index == 2:\n",
    "        selected_dataset = dataset_2\n",
    "        filename = os.path.basename(file_3d_2)\n",
    "        filename_prefix = filename.split(\".nc\")[0]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset_index. Please choose 1 or 2.\")\n",
    "    \n",
    "    return selected_dataset, filename_prefix\n",
    "\n",
    "# Assign the chosen variable group and construct a filename suffix for the output.\n",
    "if var_group_index == 1:\n",
    "    variable_group = variable_group_1\n",
    "    filename_suffix = \"-wdir-wspeed\"\n",
    "elif var_group_index == 2:\n",
    "    variable_group = variable_group_2\n",
    "    filename_suffix = \"-u-v-w\"\n",
    "else:\n",
    "    raise ValueError(\"Invalid var_group_index. Please choose 1 or 2.\")\n",
    "\n",
    "# Get the selected dataset and its corresponding filename prefix.\n",
    "dataset, filename_prefix = get_dataset_and_filename_prefix(dataset_index=ds_index)\n",
    "\n",
    "# Get the unique dimensions for the selected variable group.\n",
    "unique_dimensions_group = get_unique_dimensions(variable_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2263c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Title (Part): konstanz_4096x4096_v9_Baseline.00\n",
      "Output Filename Prefix: konstanz_4096x4096_v9_Baseline_av_3d_N03.000\n",
      "Output Filename Suffix: -wdir-wspeed\n",
      "Variables selected for export: ['wdir', 'wspeed']\n",
      "Unique dimensions for selected variables: ['y', 'x', 'zu_3d', 'time']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the title of the selected dataset.\n",
    "# The title typically contains simulation run information.\n",
    "print(f\"Dataset Title (Part): {dataset.title.split(' ')[4]}\") # Example: Extracts a specific part of the title string.\n",
    "\n",
    "# Print the constructed filename prefix and suffix for the output NetCDF file.\n",
    "print(f\"Output Filename Prefix: {filename_prefix}\")\n",
    "print(f\"Output Filename Suffix: {filename_suffix}\")\n",
    "\n",
    "# Print the list of variables included in the selected group.\n",
    "print(f\"Variables selected for export: {variable_group}\")\n",
    "\n",
    "# Print the unique dimensions identified for the selected variable group.\n",
    "# This confirms the dimensional structure that will be used for the output NetCDF file.\n",
    "print(f\"Unique dimensions for selected variables: {unique_dimensions_group}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db2009a",
   "metadata": {},
   "source": [
    "## 7. Perform Extraction and Save Data\n",
    "This final and crucial section performs the actual extraction of surface-level 2D data from the 3D variables. It iterates through the selected time steps, extracts the lowest non-zero atmospheric layer and the layer above it for each variable using the subset_data function, and then writes this processed 2D data into a new NetCDF file. This allows for focused analysis on atmospheric conditions near the ground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48c298c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted and saved surface data to: ./output/_02_2D_surface_from_3D/konstanz_4096x4096_v9_Baseline_av_3d_N03.000-wdir-wspeed.nc\n"
     ]
    }
   ],
   "source": [
    "# Set the time sequence to process all available time steps.\n",
    "# This ensures that surface data is extracted for every time point in the simulation.\n",
    "time_sequence = time_sequence_all\n",
    "\n",
    "# Define the output directory path.\n",
    "output_directory = \"./output/_02_2D_surface_from_3D/\"\n",
    "\n",
    "# Create the output directory if it does not already exist.\n",
    "# `os.makedirs` creates all intermediate directories. `exist_ok=True` prevents an error if the directory exists.\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Construct the full output file path.\n",
    "# Combines the directory, filename prefix, and filename suffix.\n",
    "output_filepath = os.path.join(output_directory, f\"{filename_prefix}{filename_suffix}.nc\")\n",
    "\n",
    "# --- Added check to skip export if file already exists ---\n",
    "if os.path.exists(output_filepath):\n",
    "    print(f\"Skipping export: File already exists at {output_filepath}\")\n",
    "else:\n",
    "    # Create a new NetCDF file in write mode (\"w\").\n",
    "    # `nc.Dataset` is used for creating and writing to NetCDF files.\n",
    "    with nc.Dataset(output_filepath, mode=\"w\", format='NETCDF4_CLASSIC') as out_file:\n",
    "        # --- Copy Global Attributes ---\n",
    "        # Iterate through all global attributes of the input dataset.\n",
    "        for attr_name in dataset.ncattrs():\n",
    "            # Exclude \"VAR_LIST\" from direct copying as it will be reconstructed.\n",
    "            if attr_name != \"VAR_LIST\":\n",
    "                # Copy each attribute's value to the output file.\n",
    "                out_file.setncattr(attr_name, dataset.getncattr(attr_name))\n",
    "        \n",
    "        # Reconstruct and set the \"VAR_LIST\" attribute for the output file.\n",
    "        # This attribute lists the variables contained within the new file, formatted as ';var1;var2;'.\n",
    "        var_list_str = \"\".join([f\";{var}\" for var in variable_group]) + \";\"\n",
    "        out_file.setncattr('VAR_LIST', var_list_str)\n",
    "        \n",
    "        # --- Create Dimensions ---\n",
    "        # Iterate through the unique dimensions required by the `variable_group`.\n",
    "        for dim_name in unique_dimensions_group:\n",
    "            # Handling for 'time' dimension:\n",
    "            if dim_name == 'time':\n",
    "                # Create 'time' dimension in the output file with the length of the processed time sequence.\n",
    "                out_file.createDimension(dim_name, len(time_sequence))\n",
    "                # Create a variable for 'time' and copy its data from the original dataset,\n",
    "                # but only for the `time_sequence` (if `time_sequence` is a subset of `time_sequence_all`).\n",
    "                # In this notebook, `time_sequence = time_sequence_all`, so all original times are copied.\n",
    "                out_variable_dim = out_file.createVariable(dim_name, dataset[dim_name].dtype, (dim_name,))\n",
    "                out_variable_dim[:] = dataset[dim_name][:]\n",
    "            \n",
    "            # Handling for 'z' (vertical) dimension:\n",
    "            elif dim_name.startswith('z'):\n",
    "                # For surface extraction, the 'z' dimension is reduced to a size of 1.\n",
    "                # This represents the chosen surface layer.\n",
    "                dim_size = 1 \n",
    "                out_file.createDimension(dim_name, dim_size)\n",
    "                # Create a variable for 'z' and copy only the first level's value (as we take slices from 3D).\n",
    "                out_variable_dim = out_file.createVariable(dim_name, dataset[dim_name].dtype, (dim_name,))\n",
    "                out_variable_dim[:] = dataset[dim_name][:1] # Copy only the first z-level's coordinate.\n",
    "\n",
    "            # Handling for 'x' and 'y' (horizontal) dimensions:\n",
    "            elif dim_name.startswith('x') or dim_name.startswith('y'):\n",
    "                # These dimensions retain their original sizes.\n",
    "                out_file.createDimension(dim_name, len(dataset.dimensions[dim_name]))\n",
    "                out_variable_dim = out_file.createVariable(dim_name, dataset[dim_name].dtype, (dim_name,))\n",
    "                out_variable_dim[:] = dataset[dim_name][:]\n",
    "                \n",
    "            # Handling for 'lon' and 'lat' (geographic coordinates) dimensions:\n",
    "            elif dim_name.startswith('lon') or dim_name.startswith('lat'):\n",
    "                # These dimensions retain their original sizes.\n",
    "                out_file.createDimension(dim_name, len(dataset.dimensions[dim_name]))\n",
    "                out_variable_dim = out_file.createVariable(dim_name, dataset[dim_name].dtype, (dim_name,))\n",
    "                out_variable_dim[:] = dataset[dim_name][:]\n",
    "            \n",
    "        # --- Write Variable Values ---\n",
    "        # Iterate through each variable name in the selected `variable_group`.\n",
    "        for variable_name in variable_group:\n",
    "            # Check if the variable exists in the input dataset.\n",
    "            if variable_name in dataset.variables:\n",
    "                # Create the variable in the output NetCDF file.\n",
    "                # The dimensions are copied from the original variable, but the `z` dimension's size will effectively be 1.\n",
    "                # Note: `dataset[variable_name].dimensions` will include 'z', which is handled by its dimension size of 1.\n",
    "                out_variable = out_file.createVariable(variable_name,\n",
    "                                                       dataset[variable_name].dtype, \n",
    "                                                       dataset[variable_name].dimensions, # Use original dimensions for consistency\n",
    "                                                       fill_value=dataset[variable_name]._FillValue if '_FillValue' in dataset[variable_name].ncattrs() else -9999.0) # Copy fill value\n",
    "\n",
    "                # Loop through each time index in the `time_sequence`.\n",
    "                for t_idx, time_step_idx in enumerate(time_sequence):\n",
    "                    # Use `subset_data` to extract the two 2D surface slices for the current time step.\n",
    "                    # `data_0` (lowest non-zero layer) and `data_1` (next layer).\n",
    "                    data_0, data_1 = subset_data(dataset, variable_name, time_step_idx)\n",
    "                    \n",
    "                    # Write `data_1` (the next atmospheric layer above the lowest non-zero) to the output file.\n",
    "                    # `[t_idx, 0, :, :]` specifies writing to the `t_idx`-th time step, 0-th z-layer, and all y, x dimensions.\n",
    "                    # To save `data_0` instead, change `data_1` to `data_0`.\n",
    "                    out_variable[t_idx, 0, :, :] = data_1\n",
    "            else:\n",
    "                print(f\"Warning: Variable '{variable_name}' not found in the dataset. Skipping export for this variable.\")\n",
    "\n",
    "    print(f\"Successfully extracted and saved surface data to: {output_filepath}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coklimax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
